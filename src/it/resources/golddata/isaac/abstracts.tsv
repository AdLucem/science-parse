d9f69149832fa4ceb0ab3b0285863ae2c8977d2e	Possibilistic answer set programming (PASP) extends answer set programming (ASP) by attaching to each rule a degree of certainty. While such an extension is important from an application point of view, existing seman- tics are not well-motivated, and do not al- ways yield intuitive results. To develop a more suitable semantics, we first introduce a characterization of answer sets of classi- cal ASP programs in terms of possibilistic logic where an ASP program specifies a set of constraints on possibility distributions. This characterization is then naturally generalized to define answer sets of PASP programs. We furthermore provide a syntactic counterpart, leading to a possibilistic generalization of the well-known Gelfond-Lifschitz reduct, and we show how our framework can readily be im- plemented using standard ASP solvers.
bcae858633935c727739a73447b50b40b7c52794	The issue of single range based observability analysis and observer design for the kinematics model of a 3D vehicle eventually subject to a constant unknown drift velocity is addressed. The proposed method departs from alternative solutions to the problem and leads to the definition of a linear time invariant state equation with a linear time varying output that can be used to globally solve the original nonlinear state estimation with a standard Kalman filter. Simple necessary and sufficient observability conditions are derived. Numerical simulation examples are described to illustrate the performance of the method.
5f17cac51538fc860379e2a4887586757be6182e	In contrast to the classical cyclic prefix (CP)-OFDM, the time domain synchronous (TDS)-OFDM employs a known pseudo noise (PN) sequence as guard interval (GI). Conventional channel estimation methods for TDS-OFDM are based on the exploitation of the PN sequence and consequently suffer from intersymbol interference (ISI). This paper proposes a novel dataaided channel estimation method which combines the channel estimates obtained from the PN sequence and, most importantly, additional channel estimates extracted from OFDM data symbols. Data-aided channel estimation is carried out using the rebuilt OFDM data symbols as virtual training sequences. In contrast to the classical turbo channel estimation, interleaving and decoding functions are not included in the feedback loop when rebuilding OFDM data symbols thereby reducing the complexity. Several improved techniques are proposed to refine the data-aided channel estimates, namely one-dimensional (1- D)/two-dimensional (2-D) moving average and Wiener filtering. Finally, the MMSE criteria is used to obtain the best combination results and an iterative process is proposed to progressively refine the estimation. Both MSE and BER simulations using specifications of the DTMB system are carried out to prove the effectiveness of the proposed algorithm even in very harsh channel conditions such as in the single frequency network (SFN) case.
6ee7d70f2dbfc0d45fbf20485f82a9ed7e175725	Convolutional neural nets (convnets) trained from massive labeled datasets [1] have substantially improved the state-of-the-art in image classification [2] and object detection [3]. However, visual understanding requires establishing correspondence on a finer level than object category. Given their large pooling regions and training from whole-image labels, it is not clear that convnets derive their success from an accurate correspondence model which could be used for precise localization. In this paper, we study the effectiveness of convnet activation features for tasks requiring correspondence. We present evidence that convnet features localize at a much finer scale than their receptive field sizes, that they can be used to perform intraclass alignment as well as conventional hand-engineered features, and that they outperform conventional features in keypoint prediction on objects from PASCAL VOC 2011 [4].
d7e464a1e466fb04e72a961c24d10ecf65c20890	A number of representation schemes have been presented for use within Learning Classifier Systems, ranging from binary encodings to neural networks. This paper presents results from an investigation into using discrete and fuzzy dynamical system representations within the XCSF Learning Classifier System. In particular, asynchronous Random Boolean Networks are used to represent the traditional condition-action production system rules in the discrete case and asynchronous Fuzzy Logic Networks in the continuous-valued case. It is shown possible to use self-adaptive, open-ended evolution to design an ensemble of such dynamical systems within XCSF to solve a number of well-known test problems.
41b3a5272d0c8f98b73e7275481cd917802ad8b7	In this paper, we strengthen the competitive analysis results obtained for a fundamental online streaming problem, the Frequent Items Problem. Additionally, we contribute with a more detailed analysis of this problem, using alternative performance measures, supplementing the insight gained from competitive analysis. The results also contribute to the general study of performance measures for online algorithms. It has long been known that competitive analysis suffers from drawbacks in certain situations, and many alternative measures have been proposed. However, more systematic comparative studies of performance measures have been initiated recently, and we continue this work, using competitive analysis, relative interval analysis, and relative worst order analysis on the Frequent Items Problem.
01ce0903206717ac40f9a26ce9478bdeff5c1262	In this paper, we propose modulation diversity techniques for Spatial Modulation (SM) system using Complex Interleaved Orthogonal Design (CIOD) meant for two transmit antennas. Specifically, we show that by using the CIOD for two transmit antenna system, the standard SM scheme, where only one transmit antenna is activated in any symbol duration, can achieve a transmit diversity order of two. We show with our simulation results that the proposed schemes offer transmit diversity order of two, and hence, give a better Symbol Error Rate performance than the SM scheme with transmit diversity order of one.
3521f22e34fef8a53d55df180a76df5a7a4e7f87	The rise of the social media sites, such as blogs, wikis, Digg and Flickr among others, underscores the transformation of the Web to a participatory medium in which users are collaboratively creating, evaluating and distributing information. The innovations introduced by social media has lead to a new paradigm for interacting with information, what we call ’social information processing’. In this paper, we study how social news aggregator Digg exploits social information processing to solve the problems of document recommendation and rating. First, we show, by tracking stories over time, that social networks play an important role in document recommendation. The second contribution of this paper consists of two mathematical models. The first model describes how collaborative rating and promotion of stories emerges from the independent decisions made by many users. The second model describes how a user’s influence, the number of promoted stories and the user’s social network, changes in time. We find qualitative agreement between predictions of the model and user data gathered from Digg.
15aa277b1054cdcdf7fc018e3a3abe2df7a1691b	Temporal-difference (TD) networks are a class of predictive state representations that use well-established TD methods to learn models of partially observable dynamical systems. Previous research with TD networks has dealt only with dynamical systems with finite sets of observations and actions. We present an algorithm for learning TD network representations of dynamical systems with continuous observations and actions. Our results show that the algorithm is capable of learning accurate and robust models of several noisy continuous dynamical systems. The algorithm presented here is the first fully incremental method for learning a predictive representation of a continuous dynamical system.
8e924eec08583abd0fdabb45a56346ab4f467aa1	The cut-elimination method CERES (for first- and higherorder classical logic) is based on the notion of a characteristic clause set, which is extracted from an LK-proof and is always unsatisfiable. A resolution refutation of this clause set can be used as a skeleton for a proof with atomic cuts only (atomic cut normal form). This is achieved by replacing clauses from the resolution refutation by the corresponding projections of the original proof. We present a generalization of CERES (called CERESs) to first-order proof schemata and define a schematic version of the sequent calculus called LKSE , and a notion of proof schema based on primitive recursive definitions. A method is developed to extract schematic characteristic clause sets and schematic projections from these proof schemata. We also define a schematic resolution calculus for refutation of schemata of clause sets, which can be applied to refute the schematic characteristic clause sets. Finally the projection schemata and resolution schemata are plugged together and a schematic representation of the atomic cut normal forms is obtained. A major benefit of CERESs is the extension of cut-elimination to inductively defined proofs: we compare CERESs with standard calculi using induction rules and demonstrate that CERESs is capable of performing cut-elimination where traditional methods fail. The algorithmic handling of CERESs is supported by a recent extension of the CERES system.
557e25cedbf8f9d55ae1a8919eb109dd96dedaa5	A common approach to distributed control design is to impose sparsity constraints on the controller structure. Such constraints, however, may greatly complicate the control design procedure. This paper puts forward an alternative structure, which is not sparse yet might nevertheless be well suited for distributed control purposes. The structure appears as the optimal solution to a class of coordination problems arising in multi-agent applications. The controller comprises a diagonal (decentralized) part, complemented by a rank-one coordination term. Although this term relies on information about all subsystems, its implementation only requires a simple averaging operation.
6f9167ddb392a43a7e36a2df1feefa184d82763e	Recently an increasing amount of research is devoted to the question of how the most influential nodes (seeds) can be found effectively in a complex network. There are a number of measures proposed for this purpose, for instance, high-degree centrality measure reflects the importance of the network topology and has a reasonable runtime performance to find a set of nodes with highest degree, but they do not have a satisfactory dissemination potentiality in the network due to having many common neighbors (CN(1)) and common neighbors of neighbors (CN(2)). This flaw holds in other measures as well. In this paper, we compare high-degree centrality measure with other well-known measures using ten datasets in order to find a proportion for the common seeds in the seed sets obtained by them. We, thereof, propose an improved high-degree centrality measure (named DegreeDistance) and improve it to enhance accuracy in two phases, FIDD and SIDD, by putting a threshold on the number of common neighbors of already-selected seed nodes and a non-seed node which is under investigation to be selected as a seed as well as considering the influence score of seed nodes directly or through their common neighbors over the non-seed node. To evaluate the accuracy and runtime performance of DegreeDistance, FIDD, and SIDD, they are applied to eight large-scale networks and it finally turns out that SIDD dramatically outperforms other well-known measures and evinces comparatively more accurate performance in identifying the most influential nodes.
2774393ecb042926ba7fa6957841853ffff0396d	Recent work on loglinear models in probabilistic constraint logic programming is applied to firstorder probabilistic reasoning. Probabilities are defined directly on the proofs of atomic formulae, and by marginalisation on the atomic formulae themselves. We use Stochastic Logic Programs (SLPs) composed of labelled and unlabelled definite clauses to define the proof probabilities. We have a conservative extension of first-order reasoning, so that, for example, there is a one-one mapping between logical and random variables. We show how, in this framework, Inductive Logic Programming (ILP) can be used to induce the features of a loglinear model from data. We also compare the presented framework with other approaches to first-order probabilistic reasoning.
8c1c76248e128c7e5789c7f0a9a89fbba17e4c11	I describe a simple modification which can be applied to any citation count-based index (e.g. Hirsch’s h-index) quantifying a researcher’s publication output. The key idea behind the proposed approach is that the merit for the citations of a paper should be distributed amongst its authors according to their relative contributions. In addition to producing inherently fairer metrics I show that the proposed modification has the potential to partially normalize for the unfair effects of honorary authorship and thus discourage this practice.
198dcf518298e0afe39d005da5320cfe840480c1	In this paper the relation between nonanticipative rate distortion function (RDF) and Bayesian filtering theory is further investigated on general Polish spaces. The relation is established via an optimization on the space of conditional distributions of the so-called directed information subject to fidelity constraints. Existence of the optimal reproduction distribution of the nonanticipative RDF is shown using the topology of weak convergence of probability measures. Subsequently, we use the solution of the nonanticipative RDF to present the realization of a multidimensional partially observable source over a scalar Gaussian channel. We show that linear encoders are optimal, establishing joint source-channel coding in real-time.
13050adb7aa8aaf2e1c38f2b2c7e3d070358d261	In this paper, we present the first snap-stabilizing message forwarding protocol that uses a number of buffers per node being independent of any global parameter, that is 4 buffers per link. The protocol works on a linear chain of nodes, that is possibly an overlay on a largescale and dynamic system, e.g., Peer-to-Peer systems, Grids. . . Provided that the topology remains a linear chain and that nodes join and leave “neatly”, the protocol tolerates topology changes. We expect that this protocol will be the base to get similar results on more general topologies
5c7eebc8ba8fe8df00f54496ab743ede61314419	The Bayesian framework is a well-studied and successful framework for inductive reasoning, which includes hypothesis testing and confirmation, parameter estimation, sequence prediction, classification, and regression. But standard statistical guidelines for choosing the model class and prior are not always available or can fail, in particular in complex situations. Solomonoff completed the Bayesian framework by providing a rigorous, unique, formal, and universal choice for the model class and the prior. I discuss in breadth how and in which sense universal (non-i.i.d.) sequence prediction solves various (philosophical) problems of traditional Bayesian sequence prediction. I show that Solomonoff’s model possesses many desirable properties: Strong total and future bounds, and weak instantaneous bounds, and in contrast to most classical continuous prior densities has no zero p(oste)rior problem, i.e. can confirm universal hypotheses, is reparametrization and regrouping invariant, and avoids the old-evidence and updating problem. It even performs well (actually better) in non-computable environments.
ddf1ea128fbc14a203c3b3d44d0135bb4dc33ffe	Since its introduction by Gauss, Matrix Algebra has facilitated understanding of scientific problems, hiding distracting details and finding more elegant and efficient ways of computational solving. Todays largest problems, which often originate from multidimensional data, might profit from even higher levels of abstraction. We developed a framework for solving tensor structured problems with tensor algebra that unifies concepts from tensor analysis, multilinear algebra and multidimensional signal processing. In contrast to the conventional matrix approach, it allows the formulation of multidimensional problems, in a multidimensional way, preserving structure and data coherence; and the implementation of automated optimizations of solving algorithms, based on the commutativity of all tensor operations. Its ability to handle large scientific tasks is showcased by a real-world, 4D medical imaging problem, with more than 30 million unknown parameters solved on a current, inexpensive hardware. This significantly surpassed the best published matrix-based approach.
b5e6da04c35a586609a46bbbd7b1ad031a658b08	This work presents a distributed method for control centers to monitor the operating condition of a power network, i.e., to estimate the network state, and to ultimately determine the occurrence of threatening situations. State estimation has been recognized to be a fundamental task for network control centers to ensure correct and safe functionalities of power grids. We consider (static) state estimation problems, in which the state vector consists of the voltage magnitude and angle at all network buses. We consider the state to be linearly related to network measurements, which include power flows, current injections, and voltages phasors at some buses. We admit the presence of several cooperating control centers, and we design two distributed methods for them to compute the minimum variance estimate of the state given the network measurements. The two distributed methods rely on different modes of cooperation among control centers: in the first method an incremental mode of cooperation is used, whereas, in the second method, a diffusive interaction is implemented. Our procedures, which require each control center to know only the measurements and structure of a subpart of the whole network, are computationally efficient and scalable with respect to the network dimension, provided that the number of control centers also increases with the network cardinality. Additionally, a finite-memory approximation of our diffusive algorithm is proposed, and its accuracy is characterized. Finally, our estimation methods are exploited to develop a distributed algorithm to detect corrupted data among the network measurements.
546f8d1fbae5e8a7a20195efacea24bd3183b708	This paper studies the interaction between knowledge, time and coordination in systems in which timing information is available. Necessary conditions are given for the causal structure in coordination problems consisting of orchestrating a set of actions in a manner that satisfies a variety of temporal ordering assumptions. Results are obtained in two main steps: A specification of coordination is shown to require epistemic properties, and the causal structure required to obtain these properties is characterised via “knowledge gain” theorems. A new causal structure called a centibroom structure is presented, generalising previous causal structures for this model. It is shown to capture coordination tasks in which a sequence of clusters of events is performed in linear order, while within each cluster all actions must take place simultaneously. This form of coordination is shown to require the agents to gain a nested common knowledge of particular facts, which in turn requires a centibroom. Altogether, the results presented provide a broad view of the causal shape underlying partially ordered coordinated actions. This, in turn, provides insight into and can enable the design of efficient solutions to the coordination tasks in question.
41c86d417baa3c67b6a027fb8bd3784ea31a3b8e	This article describes an algorithm for reducing the intermediate alphabets in cascades of finite-state transducers (FSTs). Although the method modifies the component FSTs, there is no change in the overall relation described by the whole cascade. No additional information or special algorithm, that could decelerate the processing of input, is required at runtime. Two examples from Natural Language Processing are used to illustrate the effect of the algorithm on the sizes of the FSTs and their alphabets. With some FSTs the number of arcs and symbols shrank considerably
2e939ed3bb378ea966bf9f710fc1138f4e16ef38	Conditional Value at Risk (CVaR) is a prominent risk measure that is being used extensively in various domains. We develop a new formula for the gradient of the CVaR in the form of a conditional expectation. Based on this formula, we propose a novel sampling-based estimator for the gradient of the CVaR, in the spirit of the likelihood-ratio method. We analyze the bias of the estimator, and prove the convergence of a corresponding stochastic gradient descent algorithm to a local CVaR optimum. Our method allows to consider CVaR optimization in new domains. As an example, we consider a reinforcement learning application, and learn a risksensitive controller for the game of Tetris.
3717dc0dba9fdb13d1459ed4edf7955dce2e06b3	A theoretic framework for multimedia information retrieval is introduced which guarantees optimal retrieval effectiveness. In particular, a Ranking Principle for Distributed Multimedia-Documents (RPDM) is described together with an algorithm that satisfies this principle. Finally, the RPDM is shown to be a generalization of the Probability Ranking principle (PRP) which guarantees optimal retrieval effectiveness in the case of text document retrieval. The PRP justifies theoretically the relevance ranking adopted by modern search engines. In contrast to the classical PRP, the new RPDM takes into account transmission and inspection time, and most importantly, aspectual recall rather than simple recall.
5c55cc6b449bcc6b8a6cf189452135796e7a0346	In this paper, we are interested in the number of fixed points of functions f : An → An over a finite alphabet A defined on a given signed digraph D. We first use techniques from network coding to derive some lower bounds on the number of fixed points that only depends on D. We then discover relationships between the number of fixed points of f and problems in coding theory, especially the design of codes for the asymmetric channel. Using these relationships, we derive upper and lower bounds on the number of fixed points, which significantly improve those given in the literature. We also unveil some interesting behaviour of the number of fixed points of functions with a given signed digraph when the alphabet varies. We finally prove that signed digraphs with more (disjoint) positive cycles actually do not necessarily have functions with more fixed points.
9e59d03fbb534832ced523250ee429f41893ab39	Commutative encryption is a useful but rather strict notion in cryptography. In this paper, we define a loose variation of commutative encryption-commutative-like encryption and give an example: the generalization of ElGamal scheme. The application of the new variation is also discussed.
a98488969aed4d6add1115ce18c19c89b4826a92	The synchrosqueezing method aims at decomposing 1D functions as superpositions of a small number of “Intrinsic Modes”, supposed to be well separated both in time and frequency. Based on the unidimensional wavelet transform and its reconstruction properties, the synchrosqueezing transform provides a powerful representation of multicomponent signals in the time-frequency plane, together with a reconstruction of each mode. In this paper, a bidimensional version of the synchrosqueezing transform is defined, by considering a well–adapted extension of the concept of analytic signal to images: the monogenic signal. The natural bidimensional counterpart of the notion of Intrinsic Mode is then the concept of “Intrinsic Monogenic Mode” that we define. Thereafter, we investigate the properties of its associated Monogenic Wavelet Decomposition. This leads to a natural bivariate extension of the Synchrosqueezed Wavelet Transform, for decomposing and processing multicomponent images. Numerical tests validate the effectiveness of the method for different examples.
b5a189b46ae26c36de0fd78050f2490a786057bc	Semi-supervised support vector machines (S3VMs) are a kind of popular approaches which try to improve learning performance by exploiting unlabeled data. Though S3VMs have been found helpful in many situations, they may degenerate performance and the resultant generalization ability may be even worse than using the labeled data only. In this paper, we try to reduce the chance of performance degeneration of S3VMs. Our basic idea is that, rather than exploiting all unlabeled data, the unlabeled instances should be selected such that only the ones which are very likely to be helpful are exploited, while some highly risky unlabeled instances are avoided. We propose the S3VM-us method by using hierarchical clustering to select the unlabeled instances. Experiments on a broad range of data sets over eighty-eight different settings show that the chance of performance degeneration of S3VM-us is much smaller than that of existing S3VMs.
bdb1b4128730838eb2fed83829f46a9077eca9f7	Transportation processes, which play a prominent role in the life and social sciences, are typically described by discrete models on lattices. For studying their dynamics a continuous formulation of the problem via partial differential equations (PDE) is employed. In this paper we propose a symbolic computation approach to derive mean-field PDEs from a latticebased model. We start with the microscopic equations, which state the probability to find a particle at a given lattice site. Then the PDEs are formally derived by Taylor expansions of the probability densities and by passing to an appropriate limit as the time steps and the distances between lattice sites tend to zero. We present an implementation in a computer algebra system that performs this transition for a general class of models. In order to rewrite the mean-field PDEs in a conservative formulation, we adapt and implement symbolic integration methods that can handle unspecified functions in several variables. To illustrate our approach, we consider an application in crowd motion analysis where the dynamics of bidirectional flows are studied. However, the presented approach can be applied to various transportation processes of multiple species with variable size in any dimension, for example, to confirm several proposed mean-field models for cell motility.
520633c68777988873f5aa011df45a5289c04217	We describe a pure divide-and-conquer parallel algorithm for computing 3D convex hulls. We implement that algorithm on GPU hardware, and find a significant speedup over comparable CPU implementations.
31368c6398a34b489f78708039177d858b171d13	Leading agent-based trust models address two important needs. First, they show how an agent may estimate the trustworthiness of another agent based on prior interactions. Second, they show how agents may share their knowledge in order to cooperatively assess the trustworthiness of others. However, in real-life settings, information relevant to trust is usually obtained piecemeal, not all at once. Unfortunately, the problem of maintaining trust has drawn little attention. Existing approaches handle trust updates in a heuristic, not a principled, manner. This paper builds on a formal model that considers probability and certainty as two dimensions of trust. It proposes a mechanism using which an agent can update the amount of trust it places in other agents on an ongoing basis. This paper shows via simulation that the proposed approach (a) provides accurate estimates of the trustworthiness of agents that change behavior frequently; and (b) captures the dynamic behavior of the agents. This paper includes an evaluation based on a real dataset drawn from Amazon Marketplace, a leading e-commerce site.
1ad4974c4d79b00c890bd2dd1562600bd9c7e2bd	In a significant minority of cases, certain pronouns, especially the pronoun it, can be used without referring to any specific entity. This phenomenon of pleonastic pronoun usage poses serious problems for systems aiming at even a shallow understanding of natural language texts. In this paper, a novel approach is proposed to identify such uses of it: the extrapositional cases are identified using a series of queries against the web, and the cleft cases are identified using a simple set of syntactic rules. The system is evaluated with four sets of news articles containing 679 extrapositional cases as well as 78 cleft constructs. The identification results are comparable to those obtained by human efforts.
b58e020f6c9c834ba83f07322adc75f3756e087f	In the past few years, Reddit – a community-driven platform for submitting, commenting and rating links and text posts – has grown exponentially, from a small community of users into one of the largest online communities on the Web. To the best of our knowledge, this work represents the most comprehensive longitudinal study of Reddit’s evolution to date, studying both (i) how user submissions have evolved over time and (ii) how the community’s allocation of attention and its perception of submissions have changed over 5 years based on an analysis of almost 60 million submissions. Our work reveals an ever-increasing diversification of topics accompanied by a simultaneous concentration towards a few selected domains both in terms of posted submissions as well as perception and attention. By and large, our investigations suggest that Reddit has transformed itself from a dedicated gateway to the Web to an increasingly self-referential community that focuses on and reinforces its own user-generated image- and textual content over external sources.
05d1cee851e2d900ab78b3542237af4db84590fd	A class of queueing networks which consist of single-server fork-join nodes with infinite buffers is examined to derive a representation of the network dynamics in terms of max-plus algebra. For the networks, we present a common dynamic state equation which relates the departure epochs of customers from the network nodes in an explicit vector form determined by a state transition matrix. We show how the matrix may be calculated from the service time of customers in the general case, and give examples of matrices inherent in particular networks.
588bef69f7e78fea84fc0ad7bb9e85f13dd61217	In the first of this pair of papers, it was proven that there cannot be a physical computer to which one can properly pose any and all computational tasks concerning the physical universe. It was then further proven that no physical computer C can correctly carry out all computational tasks that can be posed to C. As a particular example, this result means that no physical computer that can, for any physical system external to that computer, take the specification of that external system’s state as input and then correctly predict its future state before that future state actually occurs; one cannot build a physical computer that can be assured of correctly “processing information faster than the universe does”. These results do not rely on systems that are infinite, and/or non-classical, and/or obey chaotic dynamics. They also hold even if one uses an infinitely fast, infinitely dense computer, with computational powers greater than that of a Turing Machine. This generality is a direct consequence of the fact that a novel definition of computation — “physical computation” — is needed to address the issues considered in these papers, which concern real physical computers. While this novel definition does not fit into the traditional Chomsky hierarchy, the mathematical structure and impossibility results associated with it have parallels in the mathematics of the Chomsky hierarchy. This second paper of the pair presents a preliminary exploration of some of this mathematical structure. Analogues of Chomskian results concerning universal Turing Machines and the Halting theorem are derived, as are results concerning the (im)possibility of certain kinds of error-correcting codes. In addition, an analogue of algorithmic information complexity, “prediction complexity”, is elaborated. A task-independent bound is derived on how much the prediction complexity of a computational task can differ for two different reference universal physical computers used to solve that task, a bound similar to the “encoding” bound governing how much the algorithm information complexity of a Turing machine calculation can differ for two reference universal Turing machines. Finally, it is proven that either the Hamiltonian of our universe proscribes a certain type of computation, or prediction complexity is unique (unlike algorithmic information complexity), in that there is one and only version of it that can be applicable throughout our universe.
9c039c648b93202f920537fb88480779ee0a37e2	Online community administrators are attempting to encourage their users to contribute knowledge and resources in order to provide value to members and ensure sustainability. A large number of online communities fail mainly due to the reluctance of users to share knowledge in them. Many studies on this topic have highlighted the importance of reciprocity for knowledge contribution. However, it is unclear how reciprocity is developed and what influences its development. Motivated by this concern, this study focuses on investigating the antecedents of knowledge receivers’ reciprocity in online communities. It formulates and tests a theoretical model to explain reciprocity behaviour of knowledge receivers based on equity theory and Social Identity explanation of Deindividuation Effects (SIDE) model. Our proposed model is validated through a large-scale survey in an online forum for English language learning. The results reveal that indebtedness and community norm not only are key antecedents of intention to reciprocate but are also positively related to each other. The perceived anonymity of the online community not only has a positive effect on indebtedness and intention to reciprocate, but also has an interactive effect with community norm on intention to reciprocate. Theoretical and practical implications of this study are discussed.
3b66e64f48434d1a15cdc6ee383e925299bf56c8	The paper presents and evaluates the power of a new scheme that generates search heuristics mechanically for problems expressed using a set of functions or relations over a nite set of variables. The heuristics are extracted from a parameterized approximation scheme called Mini-Bucket elimination that allows controlled trade-oo between computation and accuracy. The heuristics are used to guide Branch-and-Bound and Best-First search. Their performance is compared on two optimization tasks: the Max-CSP task deened on deterministic databases and the Most Probable Explanation task deened on probabilistic databases. Benchmarks were random data sets as well as applications to coding and medical diagnosis problems. Our results demonstrate that the heuristics generated are eeective for both search schemes, permitting controlled trade-oo between preprocessing (for heuristic generation) and search.
80fecc3c40948d36b3de4dfffa28b62f8da52edd	This paper presents a new interaction technique for browsing large visual information bases in a collaborative environment. The ATELIER project deals with learning environments for architecture and interaction design students. Since students’ attitude is to collect large amounts of data, pictures and videos in particular, an important issue is how to keep information organized according to intuitive criteria has been seriously taken into account. This paper shows how we managed to align different approaches for browsing large image sets and describes the interfaces used to seamlessly switch from one view to another. We also implemented search facilities based both on color layout and on keywords taken from an ontology. Integration was achieved by means of physical handles, i.e., barcodes.
10a1e6233fce78a5c6bd3a40cca3e9298da55abe	Step-wise refinement is a powerful paradigm for developing a complex program from a simple program by adding features incrementally. We present the AHEAD (Algebraic Hierarchical Equations for Application Design) model that shows how step-wise refinement scales to synthesize multiple programs and multiple noncode representations. AHEAD shows that software can have an elegant, hierarchical mathematical structure that is expressible as nested sets of equations. We review a tool set that supports AHEAD. As a demonstration of its viability, we have bootstrapped AHEAD tools from equational specifications, refining Java and nonJava artifacts automatically; a task that was accomplished only by ad hoc means previously.
b8f9f7dd41837eb34793a6c5fd7703792f792288	Recently, Fan et al. proposed an anonymous multireceiver encryption scheme (FHH) , and they declared that their scheme achieves confidentiality and anonymity. In this letter, We point out that the FHH scheme does not hold the defined security properties. In particular, we state that the FHH scheme does not hold the anonymity but achieves the weaker confidentiality of IND-CPA.
95e5fe77778b2fdb3918af94d27e1bb461f66515	A key idea in coding for the broadcast channel (BC) is binning, in which the transmitter encode information by selecting a codeword from an appropriate bin (the messages are thus the bin indexes). This selection is normally done by solving an appropriate (possibly difficult) combinatorial problem. Recently it has been shown that binning for the Blackwell channel –a particular BC– can be done by iterative schemes based on Survey Propagation (SP). This method uses decimation for SP and suffers a complexity of ✂✁☎✄✝✆✟✞ . In this paper we propose a new variation of the Belief Propagation (BP) algorithm, named Reinforced BP algorithm, that turns BP into a solver. Our simulations show that this new algorithm has complexity ✠✁☎✄☛✡✌☞✎✍✏✄✝✞ . Using this new algorithm together with a non-linear coding scheme, we can efficiently achieve rates close to the border of the capacity region of the Blackwell channel.
8e0ee021514e2029430159fed8a3340bc2327b37	In this paper, we present two near-optimal methods to determine the real-time collision-free path for a mobile vehicle moving in a dynamically changing environment. The proposed designs are based on the polynomial parameterization of feasible trajectories by explicitly taking into account boundary conditions, kinematic constraints, and collision-avoidance criteria. The problems of finding optimal solutions to the parameterized feasible trajectories are then formulated with respect to a near-minimal control-energy performance index and a near-shortest distance performance index, respectively. The obtained optimal solutions are analytical and suitable for practical applications which may require real-time trajectory planning and replanning. Computer simulations are provided to validate the effectiveness of the proposed near-optimal trajectory-planning methods.
cc0b1d8e8108e7f37702ea6f649b1d15e4382a95	The ALIZ-E project’s goal is to design a robot companion able to maintain affective interactions with young users over a period of time. One of these interactions consists in teaching a dance to hospitalized children according to their capabilities. We propose a methodology for adapting both, the movements used in the dance based on the user’s cognitive and physical capabilities through a set of metrics, and the robot’s interaction based on the user’s personality traits.
0eb7343cdd90265282bd261c0e48cf2fd73ea465	Carbon nanotubes are often seen as the only alternative technology to silicon transistors. While they are the most likely short-term alternative, other longer-term alternatives should be studied as well, even if their properties are less familiar to chip designers. While contemplating biological neurons as an alternative component may seem preposterous at first sight, significant recent progress in CMOS-neuron interface suggests this direction may not be unrealistic; moreover, biological neurons are known to self-assemble into very large networks capable of complex information processing tasks, something that has yet to be achieved with other emerging technologies. The first step to designing computing systems on top of biological neurons is to build an abstract model of selfassembled biological neural networks, much like computer architects manipulate abstract models of transistors and circuits. In this article, we propose a first model of the structure of biological neural networks. We provide empirical evidence that this model matches the biological neural networks found in living organisms, and exhibits the smallworld graph structure properties commonly found in many large and self-organized systems, including biological neural networks. More importantly, we extract the simple local rules and characteristics governing the growth of such networks, enabling the development of potentially large but realistic biological neural networks, as would be needed for complex information processing/computing tasks. Based on this model, future work will be targeted to understanding the evolution and learning properties of such networks, and how they can be used to build computing systems.
e88ac0622f65a56cecc29c22a5c59eb1bdd07457	One of the key problems in model-based reinforcement learning is balancing exploration and exploitation. Another is learning and acting in large relational domains, in which there is a varying number of objects and relations between them. We provide a solution to exploring large relational Markov decision processes by developing relational extensions of the concepts of the Explicit Explore or Exploit (E 3 ) algorithm. A key insight is that the inherent generalization of learnt knowledge in the relational representation has profound implications also on the exploration strategy: what in a propositional setting would be considered a novel situation and worth exploration may in the relational setting be an instance of a well-known context in which exploitation is promising. Our experimental evaluation shows the effectiveness and benefit of relational exploration over several propositional benchmark approaches on noisy 3D simulated robot manipulation problems.
d3ba6b48f62e2fe1802efb46c3799362572eeb1d	By characterizing the worst case profile, which maximizes the content of a buffer fed with leaky bucket regulated flows in packet telecommunication networks, we derive a tight upper bound in the many-sources regime for the tail distribution of the workload generated by these flows in a FIFO queue with constant service rate. Furthermore, we compare this workload distribution with an M/G/1 queue and get insights on the better-than-Poisson property of regulated flows. We conclude that the superposition of independent regulated flows generates an asymptotically smaller workload than a marked Poisson process whose service times and intensity depend on the parameters of regulated sources.
6cf3ea2c96208d8360fa8e4187b0ddf06cffba70	Planetary exploration systems, operating under severe environmental and operating conditions, have thus far successfully employed carefully calibrated stereo cameras and manipulators to achieve desired precision in instrument placement activities. However, the environmental and functional restrictions imposed by the remote operation of semi-autonomous robots in a harsh planetary atmosphere for long periods of time limit the ultimate operational accuracy of this approach. This paper builds on an algorithm, referred to as Hybrid Image-Plane/Stereo (HIPS), developed to optimize the positioning accuracy of a manipulator. The HIPS method generates camera models through direct visual sensing of the manipulator end-effector. It estimates and subsequently uses these models to position the manipulator at a target location specified in the image-planes of a stereo camera pair using stereo correlation and triangulation. While positioning control of manipulation systems is important, orientation control of these systems is also crucial. Many planetary manipulation tasks being considered for the Mars Science Laboratory rover, due to launch in 2009, and subsequent missions, will require precise orientation control of manipulators. This paper studies the effect of using HIPS models to control the position and orientation of manipulator end-effector. As seen in previous position control experiments, the static version of the HIPS technique reduces position error by almost an order of magnitude. This paper additionally shows that orientation error is reduced by almost a factor of two.
8ab8bb8aa41151b85ab367f1152ff17cc17b87d1	A permutation graph is an intersection graph of segments lying between two parallel lines. A Seidel complementation of a finite graph at a vertex v consists in complementing the edges between the neighborhood and the non-neighborhood of v. Two graphs are Seidel complement equivalent if one can be obtained from the other by a sequence of Seidel complementations. In this paper we introduce the new concept of Seidel complementation and Seidel minor. We show that this operation preserves cographs and the structure of modular decomposition. The main contribution of this paper is to provide a new and succinct characterization of permutation graphs namely, a graph is a permutation graph if and only if it does not contain any of the following graphs: C5, C7, XF2 6 , XF2n+3 5 , C2n, n > 6 and their complements as a Seidel minor. This characterization is in a sense similar to Kuratowski’s characterization [15] of planar graphs by forbidden topological minors.
45099df43a2692bb4eea8ec12b331bb827403d57	Simulating distributed database systems is inherently difficult, as there are many factors that may influence the results. This includes architectural options as well as workload and data distribution. In this paper we present the DBsim simulator and some simulation results. The DBsim simulator architecture is extendible, and it is easy to change parameters and configuration. The simulation results in this paper is a comparison of performance and responsetimes for two concurrency control algorithms, timestamp ordering and two-phase locking. The simulations have been run with different number of nodes, network types, data declustering and workloads. The results show that for a mix of small and long transactions, the throughput is significantly higher for a system with a timestamp ordering scheduler than for a system with a two-phase locking scheduler. With only short transactions, the performance of the two schedulers are almost identical. Long transactions are treated more fair by a two-phase locking scheduler, because a timestamp ordering scheduler has a very high abort rate for long transactions.
6e5228a2c4c5d2bd25e92b2f42269c9b3e40bbdf	We present a new system, called Cirrin, for pen input of ASCII characters using word-level unistrokes. Our system addresses the tradeoff between speed and accuracy of penbased text entry by substituting precision on the part of the user for ease of recognitionon the part of the computer. Cirrin supports ease of recognition by the computer combined with natural, script-like input. This paper discusses the design space of word-level, unistroke input, focusing on the choices made in the circular model of Cirrin that is currently in daily use by the first author.
9aeb7f55d4050111022c91d86cbb436273fa79d3	Let G be an embedded planar graph whose edges may be curves. The detour between two points, p and q (on edges or vertices) of G, is the ratio between the shortest path in G between p and q and their Euclidean distance. The supremum over all pairs of points of all these ratios is called the geometric dilation of G. Our research is motivated by the problem of designing graphs of low dilation. We provide a characterization of closed curves of constant halving distance (i.e., curves for which all chords dividing the curve length in half are of constant length) which are useful in this context. We then relate the halving distance of curves to other geometric quantities such as area and width. Among others, this enables us to derive a new upper bound on the geometric dilation of closed curves, as a function of D/w, where D and w are the diameter and width, respectively. We further give lower bounds on the geometric dilation of polygons with n sides as a function of n. Our bounds are tight for centrally symmetric convex polygons.
4dae3150920e09732e0fe23134c5707bab7feb6a	We investigate using the Mondriaan matrix partitioner for unweighted graph partitioning in the communication volume and edge-cut metrics. By converting the unweighted graphs to appropriate matrices, we measure Mondriaan’s performance as a graph partitioner for the 10th DIMACS challenge on graph partitioning and clustering. We find that Mondriaan can effectively be used as a graph partitioner: w.r.t. the edge-cut metric, Mondriaan’s best results are on average within 13% of the best known results as listed in Chris Walshaw’s partitioning archive, but it is an order of magnitude slower than dedicated graph partitioners.
4b34c1f7d288eaa824e588aabddc83afc200bf62	Functional dependencies (FDs) and inclusion dependencies (INDs) are the most fundamental integrity constraints that arise in practice in relational databases. A given set of FDs does not interact with a given set of INDs if logical implication of any FD can be determined solely by the given set of FDs, and logical implication of any IND can be determined solely by the given set of INDs. We exhibit a necessary condition and two novel sucient conditions for a set of FDs and a set of proper circular INDs not to interact; these two sucient conditions are orthogonal to known results in the database literature. We also discuss the diculty in obtaining a syntactic necessary and sucient condition for no interaction between FDs and INDs.
765f6ca92d5c228847c2ceb37b756ecf980c95a4	At the birth of participatory design, there was a strong political consciousness surrounding the design of new technology, the design process in particular, establishing a rich set of methods and tools for user-centered design. Today, the term design has extended its scope of concern beyond the process of design and into how users interact with the designed product on a day-to-day basis. This paper is an attempt to call to attention the need for a new set of methods, attitudes and approaches, along with the existing, to discuss, analyze and reflect upon the politics at the interface. By presenting a critical analysis of two design cases, we elicit the importance of such an agenda and the implications for design in doing so. We use the Foucauldian notion of power to analyze the power relationships in these two cases and to articulate the politics at the interface. We conclude by emphasizing the need for furthering this agenda and outlining future work.
97a6613da7eeb0ac4f70ae2e3b793364c794e6dc	Medical image registration is a difficult problem. Not only a registration algorithm needs to capture both large and small scale image deformations, it also has to deal with global and local image intensity variations. In this paper we describe a new multiresolution elastic image registration method that challenges these difficulties in image registration. To capture large and small scale image deformations, we use both global and local affine transformation algorithms. To address global and local image intensity variations, we apply an image intensity standardization algorithm to correct image intensity variations. This transforms image intensities into a standard intensity scale, which allows highly accurate registration of medical images
40d627ad2931613b3d95d90ac29834deb7cd4b83	An algorithm for the automatic reconstruction of triangular mesh surface model from range images is presented. The optimal piecewise linear surface approximation problem is defined as: Given a set S of points uniformly sampled from a bivariate function ƒ(x,y) on a rectangular grid of dimension W×H, find a minimum triangular mesh approximating the surface with vertices anchored at a subset S’ of S, such that the deviation (the error between the approximated value and f(x, y)) at any sample point is within a given bound of ε > 0. The algorithm deploys a multi-agent resource planning approach to achieve adaptive, accurate and concise piecewise linear (triangular) approximation using the L-∞ norm. The resulting manifold triangular mesh can be directly used as 3D rendering model for visualization with controllable and guaranteed quality (by ε). Due to this dual optimality, the algorithm achieves both storage efficiency and visual quality. The error control scheme further facilitates the construction of models in multiple levels of details, which is desirable in animation and virtual reality moving scenes. Experiments with various benchmark range images from smooth functional surfaces to satellite terrain images yield succinct, accurate and visually pleasant triangular meshes. Furthermore, the independence and multiplicity of agents suggests a natural parallelism for triangulation computation, which provides a promising solution for the real-time exploration of large data sets.
03f89354a1589885fd600e9dc996bd7c6e1a6515	This paper presents a framework for investigating the relationship between both the auditory and visual modalities in speech. This framework employs intentional agents to analyse multilinear bimodal representations of speech utterances in line with an extended computational phonological model.
0e51c3ca87748c21a21bd56b49ec9bbbde143d9e	A novel component-based, service-oriented framework for distributed metacomputing is described. Adopting a provider-centric view of resource sharing, this project emphasizes lightweight software infrastructures that maintain minimal state, and interface to current and emerging distributed computing standards. Resource owners host a software backplane onto which owners, clients, or third-party resellers may load components or component-suites that deliver value added services without compromising owner security or control. Standards-based descriptions of services facilitate publication and discovery via established schemes. The architecture of the container framework, design of components, security and access control schemes, and preliminary experiences are described in this paper.
030cadedef2370bd296af07fc3324c6bb8409ba5	We propose a hull operator, the reflex-free hull, that allows us to define a 3D analogue to bays in polygons. The reflex-free hull allows a rich set of topological types, yet for polyhedral input with n edges, it remains a polyhedral set with O(n) edges. This is in contrast to other possible hull definitions that give non-planar surfaces and higher combinatorial complexity. The reflex-free hull is related to identifying cavities in computer aided design and manufacturing, but we sketch examples to indicate that computing a reflex-free hull will be a challenging problem.
aeb717fbb9aac3501236bce498cbf8b98f5d8926	For storing a word or the whole text segment, we need a huge storage space. Typically a character requires 1 Byte for storing it in memory. Compression of the memory is very important for data management. In case of memory requirement compression for text data, loseless memory compression is needed. We are suggesting a lossless memory requirement compression method for text data compression. The proposed compression method will compress the text segment or the text file based on two level approaches firstly reduction and secondly compression. Reduction will be done using a word lookup table not using traditional indexing system, then compression will be done using currently available compression methods. The word lookup table will be a part of the operating system and the reduction will be done by the operating system. According to this method each word will be replaced by an address value. This method can quite effectively reduce the size of persistent memory required for text data. At the end of the first level compression with the use of word lookup table, a binary file containing the addresses will be generated. Since the proposed method does not use any compression algorithm in the first level so this file can be compressed using the popular compression algorithms and finally will provide a great deal of data compression on purely English text data.
0846a100fcdf2406ec38df867b9ec835e1ebc1e0	While Grammar Inference (GI) has been successfully applied to many diverse domains such as speech recognition and robotics, its application to software engineering has been limited, despite wide use of context-free grammars in software systems. This paper reports current developments and future directions in the applicability of GI to software engineering, where GI is seen to offer innovative solutions to the problems of inference of domain-specific language (DSL) specifications from example DSL programs and recovery of metamodels from instance models.
27028fac2782734b4d1627c535ea8c59351ac749	Shopbots and Internet sites that help users locate the best price for a product are changing the way people shop by providing valuable information on goods and services. This paper presents a first attempt to measure the value of one piece of information: the price charged for goods and services. We first establish a theoretical limit to the value of price information for the first seller in a market that decides to sell price information to a shopbot and quantify the revenues that the seller can expect to receive. We then proceed to discuss whether and how much of this theoretical value can actually be realized in equilibrium settings.
97c4cc36e62d0ca4f5eddde679c0edec6fcb8c51	The recent emergence of Cloud Computing has drastically altered everyone’s perception of infrastructure architectures, software delivery and development models. Projecting as an evolutionary step, following the transition from mainframe computers to client/server deployment models, cloud computing encompasses elements from grid computing, utility computing and autonomic computing, into an innovative deployment architecture. This rapid transition towards the clouds, has fuelled concerns on a critical issue for the success of information systems, communication and information security. From a security perspective, a number of unchartered risks and challenges have been introduced from this relocation to the clouds, deteriorating much of the effectiveness of traditional protection mechanisms. As a result the aim of this paper is twofold; firstly to evaluate cloud security by identifying unique security requirements and secondly to attempt to present a viable solution that eliminates these potential threats. This paper proposes introducing a Trusted Third Party, tasked with assuring specific security characteristics within a cloud environment. The proposed solution calls upon cryptography, specifically Public Key Infrastructure operating in concert with SSO and LDAP, to ensure the authentication, integrity and confidentiality of involved data and communications. The solution, presents a horizontal level of service, available to all implicated entities, that realizes a security mesh, within which essential trust is maintained.
4699e2c09244f3496b1c202925618ccf732a617d	Bidimensionality theory appears to be a powerful framework for the development of metaalgorithmic techniques. It was introduced by Demaine et al. [J. ACM 2005 ] as a tool to obtain sub-exponential time parameterized algorithms for problems on H-minor free graphs. Demaine and Hajiaghayi [SODA 2005 ] extended the theory to obtain polynomial time approximation schemes (PTASs) for bidimensional problems, and subsequently improved these results to EPTASs. Fomin et. al [SODA 2010 ] established a third meta-algorithmic direction for bidimensionality theory by relating it to the existence of linear kernels for parameterized problems. In this paper we revisit bidimensionality theory from the perspective of approximation algorithms and redesign the framework for obtaining EPTASs to be more powerful, easier to apply and easier to understand. Two of the most widely used approaches to obtain PTASs on planar graphs are the LiptonTarjan separator based approach [SICOMP 1980 ], and Baker’s approach [J.ACM 1994 ]. Demaine and Hajiaghayi [SODA 2005 ] strengthened both approaches using bidimensionality and obtained EPTASs for several problems, including CONNECTED DOMINATING SET and FEEDBACK VERTEX SET. We unify the two strenghtened approaches to combine the best of both worlds. At the heart of our framework is a decomposition lemma which states that for “most” bidimensional problems, there is a polynomial time algorithm which given an H-minor-free graph G as input and an  > 0 outputs a vertex set X of size  · OP T such that the treewidth of G \ X is f(). Here, OP T is the objective function value of the problem in question and f is a function depending only on . This allows us to obtain EPTASs on (apex)-minor-free graphs for all problems covered by the previous framework, as well as for a wide range of packing problems, partial covering problems and problems that are neither closed under taking minors, nor contractions. To the best of our knowledge for many of these problems including CYCLE PACKING, VERTEX-H-PACKING, MAXIMUM LEAF SPANNING TREE, and PARTIAL r-DOMINATING SET no EPTASs on planar graphs were previously known.